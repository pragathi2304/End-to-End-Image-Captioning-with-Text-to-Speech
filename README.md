End-to-End Image Captioning with Text-to-Speech

An end-to-end deep learning system that automatically generates captions for images and converts them into spoken audio, designed to improve accessibility for visually impaired users. The project combines Computer Vision and Natural Language Processing (NLP) using a CNNâ€“LSTM Encoderâ€“Decoder architecture with real-time Text-to-Speech (TTS).

ğŸš€ Features
CNN-based Feature Extraction: Uses pre-trained models (VGG16 / InceptionV3) to extract rich visual features from images
LSTM Caption Generator: Produces grammatically correct and context-aware image captions
Text-to-Speech (gTTS): Converts generated captions into natural-sounding audio
End-to-End Pipeline: Image â†’ Text â†’ Speech with low latency
Model Evaluation: Trained on Flickr8k and evaluated using BLEU, METEOR, and CIDEr scores
Scalable Architecture: Easily extendable with attention mechanisms, transformers, or multilingual TTS

ğŸ§  Tech Stack
Programming Language: Python
Deep Learning: TensorFlow / Keras
Models: CNN (VGG16/InceptionV3), LSTM
NLP & Utils: NLTK, NumPy
Visualization: Matplotlib
TTS: Google Text-to-Speech (gTTS)

ğŸ“Š Dataset
Flickr8k â€“ 8,000 images with 40,000 human-annotated captions

âš™ï¸ How It Works
Input image is preprocessed and passed through a CNN encoder
Extracted features are fed into an LSTM decoder
Caption is generated word-by-word
Final caption is converted into speech using gTTS

ğŸ“ˆ Results
Strong caption quality with competitive BLEU & CIDEr scores
Fast inference suitable for real-time applications
Seamless integration of captioning and audio output

ğŸ”® Future Enhancements

Add attention mechanisms for better region-focused captions
Replace LSTM with Transformer-based decoders
Enable multilingual captioning and speech output
Upgrade TTS to neural speech models (Tacotron / FastSpeech)
